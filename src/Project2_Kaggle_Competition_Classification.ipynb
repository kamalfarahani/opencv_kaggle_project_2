{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Log</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods, which will be used to get training and validation data loader.\n",
    "\n",
    "You need to write a custom dataset class to load data.\n",
    "\n",
    "**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "    ....\n",
    "    ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "```\n",
    "def get_data(args1, *agrs):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import TypeAlias, Generator\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "RGB: TypeAlias = tuple[int, int, int]\n",
    "\n",
    "IMAGE_SIZE: int = 224\n",
    "\n",
    "\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    KenyanFood13Dataset is Dataset class for the Kenyan Food 13 dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        train=True,\n",
    "        transform=lambda x: x,\n",
    "        random_state: int = 42,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            train (bool): True if training data, False if validation data\n",
    "        \"\"\"\n",
    "        self.data_csv_path = f\"{data_root}/train.csv\"\n",
    "        self.images_dir = f\"{data_root}/images/images\"\n",
    "\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "        self.df = pd.read_csv(self.data_csv_path)\n",
    "        data_np = self.df.to_numpy()\n",
    "        X = data_np[:, 0]\n",
    "        y = data_np[:, 1]\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=0.2,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        if self.train:\n",
    "            self.X, self.y = self.X_train, self.y_train\n",
    "        else:\n",
    "            self.X, self.y = self.X_test, self.y_test\n",
    "\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(np.unique(y))}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[Image.Image, int] | Image.Image:\n",
    "        \"\"\"\n",
    "        Returns the image and label for training dataset and only the image for validation dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): index of the item\n",
    "        Returns:\n",
    "            tuple[Image.Image, int]: (image, label)\n",
    "        \"\"\"\n",
    "        img_id, img_class = self.X[idx], self.y[idx]\n",
    "        img = Image.open(f\"{self.images_dir}/{img_id}.jpg\")\n",
    "\n",
    "        return self.transform(img), self.class_to_idx[img_class]\n",
    "\n",
    "\n",
    "class KenyanFood13DataModule(L.LightningDataModule):\n",
    "    \"\"\"\n",
    "    KenyanFood13DataModule is LightningDataModule class for the Kenyan Food 13 dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        train_batch_size: int = 32,\n",
    "        val_batch_size: int = 32,\n",
    "        num_workers: int = 4,\n",
    "        use_augmentation: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes KenyanFood13DataModule\n",
    "\n",
    "        Args:\n",
    "            data_root (str): path to the data root\n",
    "            train_batch_size (int, optional): batch size for training. Defaults to 32.\n",
    "            val_batch_size (int, optional): batch size for validation. Defaults to 32.\n",
    "            num_workers (int, optional): number of workers. Defaults to 4.\n",
    "            use_augmentation (bool, optional): whether to use data augmentation. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_root = data_root\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.use_augmentation = use_augmentation\n",
    "\n",
    "        mean, std = get_mean_and_std(data_root)\n",
    "        self.common_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if use_augmentation:\n",
    "            self.train_transforms = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomVerticalFlip(),\n",
    "                    transforms.RandomRotation(degrees=(0, 30)),\n",
    "                    transforms.RandomGrayscale(p=0.1),\n",
    "                    transforms.RandomPerspective(\n",
    "                        distortion_scale=0.2,\n",
    "                        p=0.5,\n",
    "                    ),\n",
    "                    self.common_transforms,\n",
    "                    transforms.RandomErasing(),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.train_transforms = self.common_transforms\n",
    "\n",
    "    def setup(self, stage: str | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Loads the train and validation datasets.\n",
    "\n",
    "        Args:\n",
    "            stage (str | None, optional): stage. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.train_dataset = KenyanFood13Dataset(\n",
    "            data_root=self.data_root,\n",
    "            train=True,\n",
    "            transform=self.train_transforms,\n",
    "        )\n",
    "\n",
    "        self.val_dataset = KenyanFood13Dataset(\n",
    "            data_root=self.data_root,\n",
    "            train=False,\n",
    "            transform=self.common_transforms,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Creates and returns the training dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: training dataloader\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.train_batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Creates and returns the validation dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: validation dataloader\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.val_batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def get_mean_and_std(data_root: str) -> tuple[RGB, RGB]:\n",
    "    \"\"\"\n",
    "    Computes the mean and standard deviation of the train dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple[RGB, RGB]: (mean, std)\n",
    "    \"\"\"\n",
    "    preprocess_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = KenyanFood13Dataset(\n",
    "        data_root=data_root,\n",
    "        train=True,\n",
    "        transform=preprocess_transforms,\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=4,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    for imgs, _ in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += imgs[:, i, :, :].mean()\n",
    "            std[i] += imgs[:, i, :, :].std()\n",
    "\n",
    "    mean.div_(len(dataloader))\n",
    "    std.div_(len(dataloader))\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def kaggle_test_images(\n",
    "    data_root: str,\n",
    ") -> Generator[tuple[torch.Tensor, str], None, None]:\n",
    "    \"\"\"\n",
    "    Gets the test images from the Kaggle dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: (image_names, images)\n",
    "    \"\"\"\n",
    "    data_csv_path = f\"{data_root}/test.csv\"\n",
    "    images_dir = f\"{data_root}/images/images\"\n",
    "\n",
    "    df = pd.read_csv(data_csv_path)\n",
    "    image_names = df.to_numpy()[:, 0]\n",
    "\n",
    "    mean, std = get_mean_and_std(data_root)\n",
    "    common_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for image_name in image_names:\n",
    "        image = Image.open(f\"{images_dir}/{image_name}.jpg\")\n",
    "        yield common_transforms(image), image_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "**Define your configuration here.**\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"/kaggle/input/opencv-pytorch-classification-project-2/\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    \"\"\"\n",
    "    Describes configuration of the training process\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size: int = 32\n",
    "    epochs_count: int = 50\n",
    "    init_learning_rate: float = 0.001  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5\n",
    "    test_interval: int = 1\n",
    "    data_root: str = \"./data\"\n",
    "    num_workers: int = 4\n",
    "    device: str = \"cuda\"\n",
    "    early_stopping_patience: int = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def get_metrics(\n",
    "    y_true: torch.Tensor,\n",
    "    y_pred: torch.Tensor,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculates and returns the metrics.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): true labels\n",
    "        y_pred (torch.Tensor): predicted labels\n",
    "    Returns:\n",
    "        dict[str, float]: metrics (accuracy, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    y_true_np = y_true.cpu().numpy()\n",
    "    y_pred_np = y_pred.cpu().numpy()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true_np, y_pred_np),\n",
    "        \"precision\": precision_score(y_true_np, y_pred_np, average=\"macro\"),\n",
    "        \"recall\": recall_score(y_true_np, y_pred_np, average=\"macro\"),\n",
    "        \"f1\": f1_score(y_true_np, y_pred_np, average=\"macro\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "\n",
    "**Write the methods or classes to be used for training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from learn_lib.config import TrainingConfiguration\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: L.LightningModule,\n",
    "    data_module: L.LightningDataModule,\n",
    "    training_configuration: TrainingConfiguration,\n",
    ") -> None:\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val/acc\",\n",
    "        mode=\"max\",\n",
    "        verbose=True,\n",
    "        patience=training_configuration.early_stopping_patience,\n",
    "    )\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        monitor=\"val/acc\",\n",
    "        mode=\"max\",\n",
    "        dirpath=\"checkpoints/\",\n",
    "        filename=\"model-{epoch:02d}\",\n",
    "        auto_insert_metric_name=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=training_configuration.epochs_count,\n",
    "        log_every_n_steps=training_configuration.log_interval,\n",
    "        callbacks=[\n",
    "            early_stopping,\n",
    "            model_checkpoint,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        model=model,\n",
    "        datamodule=data_module,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "**Define your model in this section.**\n",
    "\n",
    "**You are allowed to use any pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "\n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchvision import models\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "\n",
    "class KenyanFood13Classifier(L.LightningModule):\n",
    "    \"\"\"\n",
    "    KenyanFood13Classifier is the model class for the Kenyan Food 13 Classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 13,\n",
    "        weights: str = \"DEFAULT\",\n",
    "        unfreeze_layers: int = 4,\n",
    "        learning_rate: float = 0.001,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the KenyanFood13Classifier.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int, optional): number of classes. Defaults to 13.\n",
    "            weights (str, optional): weights. Defaults to \"DEFAULT\".\n",
    "            learning_rate (float, optional): learning rate. Defaults to 0.001.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = ViTForImageClassification.from_pretrained(\n",
    "            \"google/vit-base-patch16-224\",\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for i in range(1, self.hparams.unfreeze_layers):\n",
    "            for param in self.model.vit.encoder.layer[-i].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.mean_train_accuracy = MulticlassAccuracy(\n",
    "            num_classes=self.hparams.num_classes,\n",
    "            average=\"micro\",\n",
    "        )\n",
    "        self.mean_val_accuracy = MulticlassAccuracy(\n",
    "            num_classes=self.hparams.num_classes,\n",
    "            average=\"micro\",\n",
    "        )\n",
    "\n",
    "        self.mean_train_loss = MeanMetric()\n",
    "        self.mean_val_loss = MeanMetric()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor.\n",
    "        \"\"\"\n",
    "        output = self.model(x)\n",
    "        return output.logits\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        batch: torch.Tensor,\n",
    "        batch_idx: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Training step of the model.\n",
    "\n",
    "        Args:\n",
    "            batch (torch.Tensor): input tensor.\n",
    "            batch_idx (int): batch index.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: loss tensor.\n",
    "        \"\"\"\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        pred_batch = output.detach().argmax(dim=1)\n",
    "\n",
    "        # Update metrics\n",
    "        size_of_batch = data.size(0)\n",
    "        self.mean_train_loss(loss, weight=size_of_batch)\n",
    "        self.mean_train_accuracy(pred_batch, target)\n",
    "\n",
    "        self.log(\n",
    "            \"train/batch_loss\",\n",
    "            self.mean_train_loss,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train/batch_acc\",\n",
    "            self.mean_train_accuracy,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self,\n",
    "        batch: torch.Tensor,\n",
    "        batch_idx: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Validation step of the model.\n",
    "\n",
    "        Args:\n",
    "            batch (torch.Tensor): input tensor.\n",
    "            batch_idx (int): batch index.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        pred_batch = output.detach().argmax(dim=1)\n",
    "\n",
    "        # Update metrics\n",
    "        size_of_batch = data.size(0)\n",
    "        self.mean_val_loss(loss, weight=size_of_batch)\n",
    "        self.mean_val_accuracy(pred_batch, target)\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Called at the end of each training epoch.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.log(\n",
    "            \"train/loss\",\n",
    "            self.mean_train_loss,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train/acc\",\n",
    "            self.mean_train_accuracy,\n",
    "            logger=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Called at the end of each validation epoch.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.log(\n",
    "            \"val/loss\",\n",
    "            self.mean_val_loss,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val/acc\",\n",
    "            self.mean_val_accuracy,\n",
    "            logger=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        self.log(\n",
    "            \"lr\",\n",
    "            self.trainer.optimizers[0].param_groups[0][\"lr\"],\n",
    "            logger=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> optim.Optimizer:\n",
    "        \"\"\"\n",
    "        Configures the optimizer.\n",
    "\n",
    "        Returns:\n",
    "            optim.Optimizer: optimizer\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "        )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"max\",\n",
    "            patience=3,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val/acc\",\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "**Define those methods or classes, which have  not been covered in the above sections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from learn_lib.model import KenyanFood13Classifier\n",
    "from learn_lib.data_loader import (\n",
    "    KenyanFood13Dataset,\n",
    "    KenyanFood13DataModule,\n",
    "    kaggle_test_images,\n",
    ")\n",
    "from learn_lib.config import TrainingConfiguration\n",
    "from learn_lib.train import train\n",
    "\n",
    "\n",
    "def main(data_root: str) -> None:\n",
    "    \"\"\"Trains the model\n",
    "\n",
    "    Args:\n",
    "        data_root (str): path to the data root\n",
    "    \"\"\"\n",
    "    mode = input(\"Train model? (y/n): \")\n",
    "    if mode == \"y\":\n",
    "        train_model(data_root)\n",
    "    else:\n",
    "        checkpoint = input(\"Enter checkpoint path: \")\n",
    "\n",
    "        dataset = KenyanFood13Dataset(data_root)\n",
    "        class_to_idx = dataset.class_to_idx\n",
    "\n",
    "        model = KenyanFood13Classifier.load_from_checkpoint(checkpoint)\n",
    "        model.freeze()\n",
    "        model.eval()\n",
    "\n",
    "        infer_test_images(\n",
    "            model,\n",
    "            data_root,\n",
    "            class_to_idx,\n",
    "        )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    data_root: str,\n",
    ") -> None:\n",
    "    model = KenyanFood13Classifier()\n",
    "    train_config = TrainingConfiguration()\n",
    "    train_config.early_stopping_patience = 10\n",
    "    datamodule = KenyanFood13DataModule(\n",
    "        data_root=data_root,\n",
    "        use_augmentation=True,\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        model=model,\n",
    "        training_configuration=train_config,\n",
    "        data_module=datamodule,\n",
    "    )\n",
    "\n",
    "\n",
    "def infer_test_images(\n",
    "    model: KenyanFood13Classifier,\n",
    "    data_root: str,\n",
    "    class_to_idx: dict[str, int] = None,\n",
    ") -> None:\n",
    "    device = next(model.parameters()).device\n",
    "    kaggle_test_images_gen = kaggle_test_images(data_root)\n",
    "    index_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "    result_dict = {\n",
    "        \"id\": [],\n",
    "        \"class\": [],\n",
    "    }\n",
    "\n",
    "    for image, image_name in kaggle_test_images_gen:\n",
    "        image = image.to(device)\n",
    "        output = model(image.unsqueeze(0))\n",
    "        pred = output.argmax(dim=1)\n",
    "        pred_class = index_to_class[pred.item()]\n",
    "\n",
    "        print(f\"Image name: {image_name}, Predicted class: {pred_class}\")\n",
    "\n",
    "        result_dict[\"id\"].append(image_name)\n",
    "        result_dict[\"class\"].append(pred_class)\n",
    "\n",
    "    result_df = pd.DataFrame(result_dict)\n",
    "\n",
    "    result_df.to_csv(\n",
    "        \"submission.csv\",\n",
    "        index=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
    "\n",
    "**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type                      | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | model               | ViTForImageClassification | 85.8 M | eval \n",
      "1 | mean_train_accuracy | MulticlassAccuracy        | 0      | train\n",
      "2 | mean_val_accuracy   | MulticlassAccuracy        | 0      | train\n",
      "3 | mean_train_loss     | MeanMetric                | 0      | train\n",
      "4 | mean_val_loss       | MeanMetric                | 0      | train\n",
      "--------------------------------------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "64.5 M    Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.235   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "226       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 164/164 [01:30<00:00,  1.82it/s, v_num=0, val/acc=0.786, lr=0.001]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/acc improved. New best score: 0.786\n",
      "Epoch 0, global step 164: 'val/acc' reached 0.78593 (best 0.78593), saving model to '/home/kamal/projects/deep_learning_with_pytorch/week7/checkpoints/model-00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 164/164 [01:32<00:00,  1.78it/s, v_num=0, val/acc=0.772, lr=0.001, train/acc=0.640]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 328: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 164/164 [01:31<00:00,  1.80it/s, v_num=0, val/acc=0.755, lr=0.001, train/acc=0.739]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 492: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 164/164 [01:32<00:00,  1.78it/s, v_num=0, val/acc=0.778, lr=0.001, train/acc=0.771]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 656: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 164/164 [01:35<00:00,  1.72it/s, v_num=0, val/acc=0.794, lr=0.001, train/acc=0.788]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/acc improved by 0.008 >= min_delta = 0.0. New best score: 0.794\n",
      "Epoch 4, global step 820: 'val/acc' reached 0.79358 (best 0.79358), saving model to '/home/kamal/projects/deep_learning_with_pytorch/week7/checkpoints/model-04.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 164/164 [01:35<00:00,  1.72it/s, v_num=0, val/acc=0.776, lr=0.001, train/acc=0.810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 984: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 164/164 [01:33<00:00,  1.76it/s, v_num=0, val/acc=0.769, lr=0.001, train/acc=0.815]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1148: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 164/164 [01:34<00:00,  1.74it/s, v_num=0, val/acc=0.767, lr=0.001, train/acc=0.834]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1312: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 164/164 [01:35<00:00,  1.72it/s, v_num=0, val/acc=0.802, lr=0.001, train/acc=0.829]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/acc improved by 0.008 >= min_delta = 0.0. New best score: 0.802\n",
      "Epoch 8, global step 1476: 'val/acc' reached 0.80199 (best 0.80199), saving model to '/home/kamal/projects/deep_learning_with_pytorch/week7/checkpoints/model-08.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 164/164 [01:34<00:00,  1.74it/s, v_num=0, val/acc=0.777, lr=0.001, train/acc=0.852]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1640: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 164/164 [01:35<00:00,  1.72it/s, v_num=0, val/acc=0.780, lr=0.001, train/acc=0.854]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 1804: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 164/164 [01:35<00:00,  1.72it/s, v_num=0, val/acc=0.787, lr=0.001, train/acc=0.856]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 1968: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 164/164 [01:34<00:00,  1.73it/s, v_num=0, val/acc=0.795, lr=0.001, train/acc=0.866]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 2132: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 164/164 [01:32<00:00,  1.77it/s, v_num=0, val/acc=0.804, lr=0.0001, train/acc=0.876]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/acc improved by 0.002 >= min_delta = 0.0. New best score: 0.804\n",
      "Epoch 13, global step 2296: 'val/acc' reached 0.80352 (best 0.80352), saving model to '/home/kamal/projects/deep_learning_with_pytorch/week7/checkpoints/model-13.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 164/164 [01:31<00:00,  1.79it/s, v_num=0, val/acc=0.811, lr=0.0001, train/acc=0.901]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/acc improved by 0.008 >= min_delta = 0.0. New best score: 0.811\n",
      "Epoch 14, global step 2460: 'val/acc' reached 0.81116 (best 0.81116), saving model to '/home/kamal/projects/deep_learning_with_pytorch/week7/checkpoints/model-14.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 164/164 [01:32<00:00,  1.78it/s, v_num=0, val/acc=0.807, lr=0.0001, train/acc=0.911]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 2624: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 164/164 [01:32<00:00,  1.77it/s, v_num=0, val/acc=0.813, lr=0.0001, train/acc=0.924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/acc improved by 0.002 >= min_delta = 0.0. New best score: 0.813\n",
      "Epoch 16, global step 2788: 'val/acc' reached 0.81269 (best 0.81269), saving model to '/home/kamal/projects/deep_learning_with_pytorch/week7/checkpoints/model-16.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 164/164 [01:31<00:00,  1.79it/s, v_num=0, val/acc=0.811, lr=0.0001, train/acc=0.926]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 2952: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 164/164 [01:32<00:00,  1.77it/s, v_num=0, val/acc=0.813, lr=0.0001, train/acc=0.932]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/acc improved by 0.001 >= min_delta = 0.0. New best score: 0.813\n",
      "Epoch 18, global step 3116: 'val/acc' reached 0.81346 (best 0.81346), saving model to '/home/kamal/projects/deep_learning_with_pytorch/week7/checkpoints/model-18.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 164/164 [01:33<00:00,  1.76it/s, v_num=0, val/acc=0.810, lr=0.0001, train/acc=0.929]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 3280: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 164/164 [01:31<00:00,  1.79it/s, v_num=0, val/acc=0.810, lr=0.0001, train/acc=0.932]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 3444: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 164/164 [01:32<00:00,  1.78it/s, v_num=0, val/acc=0.811, lr=0.0001, train/acc=0.935]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 3608: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 164/164 [01:32<00:00,  1.77it/s, v_num=0, val/acc=0.813, lr=0.0001, train/acc=0.933]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 3772: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 164/164 [01:31<00:00,  1.79it/s, v_num=0, val/acc=0.811, lr=1e-5, train/acc=0.932]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 3936: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 164/164 [01:32<00:00,  1.77it/s, v_num=0, val/acc=0.812, lr=1e-5, train/acc=0.941]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 4100: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 164/164 [01:33<00:00,  1.76it/s, v_num=0, val/acc=0.814, lr=1e-5, train/acc=0.938]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/acc improved by 0.001 >= min_delta = 0.0. New best score: 0.814\n",
      "Epoch 25, global step 4264: 'val/acc' reached 0.81422 (best 0.81422), saving model to '/home/kamal/projects/deep_learning_with_pytorch/week7/checkpoints/model-25.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 164/164 [01:31<00:00,  1.79it/s, v_num=0, val/acc=0.813, lr=1e-5, train/acc=0.936]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 4428: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 164/164 [01:34<00:00,  1.74it/s, v_num=0, val/acc=0.814, lr=1e-5, train/acc=0.941]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 4592: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 164/164 [01:33<00:00,  1.76it/s, v_num=0, val/acc=0.816, lr=1e-5, train/acc=0.942]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/acc improved by 0.002 >= min_delta = 0.0. New best score: 0.816\n",
      "Epoch 28, global step 4756: 'val/acc' reached 0.81575 (best 0.81575), saving model to '/home/kamal/projects/deep_learning_with_pytorch/week7/checkpoints/model-28.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 164/164 [01:31<00:00,  1.79it/s, v_num=0, val/acc=0.814, lr=1e-5, train/acc=0.937]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 4920: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 164/164 [01:35<00:00,  1.71it/s, v_num=0, val/acc=0.815, lr=1e-5, train/acc=0.944]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, global step 5084: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 164/164 [01:35<00:00,  1.73it/s, v_num=0, val/acc=0.813, lr=1e-5, train/acc=0.941]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, global step 5248: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 164/164 [01:37<00:00,  1.68it/s, v_num=0, val/acc=0.813, lr=1e-5, train/acc=0.942]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 5412: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 164/164 [01:37<00:00,  1.69it/s, v_num=0, val/acc=0.813, lr=1e-6, train/acc=0.940]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 5576: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 164/164 [01:37<00:00,  1.68it/s, v_num=0, val/acc=0.813, lr=1e-6, train/acc=0.943]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34, global step 5740: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 164/164 [01:38<00:00,  1.67it/s, v_num=0, val/acc=0.813, lr=1e-6, train/acc=0.943]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35, global step 5904: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 164/164 [01:37<00:00,  1.68it/s, v_num=0, val/acc=0.813, lr=1e-6, train/acc=0.941]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, global step 6068: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 164/164 [01:37<00:00,  1.68it/s, v_num=0, val/acc=0.813, lr=1e-7, train/acc=0.943]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37, global step 6232: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 164/164 [01:37<00:00,  1.69it/s, v_num=0, val/acc=0.813, lr=1e-7, train/acc=0.942]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val/acc did not improve in the last 10 records. Best score: 0.816. Signaling Trainer to stop.\n",
      "Epoch 38, global step 6396: 'val/acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 164/164 [01:37<00:00,  1.69it/s, v_num=0, val/acc=0.813, lr=1e-7, train/acc=0.942]\n"
     ]
    }
   ],
   "source": [
    "main(data_root=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Log [5 Points]</font>\n",
    "\n",
    "**Share your TensorBoard scalars logs here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n",
    "\n",
    "\n",
    "<font style=\"color:red\">Note:</font> In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "**Share your Kaggle profile link  with us here to score , points in  the competition.**\n",
    "\n",
    "**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n",
    "\n",
    "\n",
    "**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Kaggle profile link:** [Profile](https://www.kaggle.com/kamalfarahani)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
